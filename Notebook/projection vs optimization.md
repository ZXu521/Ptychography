ä¸‹é¢æˆ‘ç»™ä½ åšä¸€ä¸ª **æœ€æ¸…æ™°ã€æœ€ç§‘å­¦ã€ç»“æ„æœ€å®Œæ•´çš„æ€æƒ³è°±ç³»åˆ†ç±»**ã€‚
 è¿™æ˜¯ä½ ç ”ç©¶ ptychography ä»¥åŠæœªæ¥å†™ç»¼è¿°æ–‡ç« ã€å¼€ä¼š report æ—¶æœ€éœ€è¦ç†è§£çš„â€œç®—æ³•å“²å­¦åœ°å›¾â€ã€‚

æˆ‘å°†æŠŠä½ æåˆ°çš„å››å¤§ç±»æ–¹æ³•ï¼š

- **ER / HIOï¼ˆFienupï¼‰**
- **Difference Map (DM) / RAAR / HPR**
- **PIE familyï¼ˆPIE / ePIE / rPIE / AD-PIE / mPIEï¼‰**
- **ML-based ptychographyï¼ˆMLPIE / Wirtinger gradient / maximum likelihood / stochastic gradientï¼‰**

æ‹†å¼€æˆä¸¤å¤§æ€æƒ³ä½“ç³»ï¼š

# ğŸŒ• **â… . æŠ•å½±ç±»ï¼ˆProjection-basedï¼‰ç®—æ³•**

# ğŸŒ• **â…¡. ä¼˜åŒ–ç±»ï¼ˆOptimization-basedï¼‰ç®—æ³•**

å®ƒä»¬è´¯ç©¿æ•´ä¸ª phase retrieval ä¸ ptychography å‘å±•å²ã€‚

ä½ çœ‹å®Œä¸‹é¢çš„åˆ†ç±»ï¼Œå°±ä¼šå½»åº•ä¸å†æ··æ·† PIE åˆ°åº•æ˜¯ä¸æ˜¯ gradientï¼ˆä¸æ˜¯ï¼‰ï¼Œ
 MLPIE åˆ°åº•å’Œ PIE æœ‰å•¥åŒºåˆ«ï¼ˆæ ¹æœ¬æ€æƒ³ä¸åŒï¼‰ï¼Œ
 difference map åˆ°åº•å’Œ ER ä»€ä¹ˆå…³ç³»ï¼ˆéƒ½æ˜¯æŠ•å½±å®¶æ—ï¼‰ã€‚

------

# ğŸŸ¦ **â… . æŠ•å½±ç±»ï¼ˆProjection-basedï¼‰ç®—æ³•**

> åŸºæœ¬æ€æƒ³ï¼š
>  **ä¸æ–­æŠŠä¼°è®¡æ³¢åœºæŠ•å½±åˆ°ä¸åŒçš„çº¦æŸé›†åˆï¼ˆconstraint setsï¼‰ä¸Š**ï¼Œ
>  æœŸæœ›å®ƒæœ€ç»ˆè½åœ¨å¤šä¸ªçº¦æŸçš„äº¤é›†ä¸Šã€‚

è¿™æ˜¯æœ€æ—©æœŸçš„ phase retrieval æ€æƒ³ï¼ˆä» 1970 å¹´ä»£å°±æœ‰ï¼‰ã€‚
 éå¸¸å‡ ä½•åŒ–ã€æ— å…¨å±€æŸå¤±å‡½æ•°ã€‚

------

# ğŸŸ© **(1) ER / HIOï¼ˆç»å…¸ Fienup ç³»åˆ—ï¼‰â€” çº¯æŠ•å½±ï¼ˆPOCSï¼‰ç®—æ³•**

ä»£è¡¨ï¼š

- ER (Error Reduction)
- HIO (Hybrid Inputâ€“Output)
- RAARï¼ˆå³ ER ä¸åå°„çš„ç»„åˆï¼‰

æ ¸å¿ƒï¼š

- â€œé¢‘åŸŸæ¨¡çº¦æŸâ€ = ä¸€ä¸ªé›†åˆ
- â€œç©ºåŸŸæ”¯æŒçº¦æŸâ€ = ä¸€ä¸ªé›†åˆ
- åœ¨è¿™ä¸¤ä¸ªé›†åˆä¹‹é—´ä¸æ–­åšæŠ•å½±æˆ–åå°„

æ•°å­¦å½¢å¼ï¼š

[
 \Psi \leftarrow \Pi_{\text{Mod}}(\Pi_{\text{Support}}(\Psi))
 ]

ç‰¹ç‚¹ï¼š

- **æ²¡æœ‰å…¨å±€ loss å‡½æ•°**
- æ˜¯ **æŠ•å½±ç®—å­åå¤è¿­ä»£**
- å¼ºä¾èµ– convexityï¼ˆä½†å®é™…ä¸Šçº¦æŸæ˜¯éå‡¸çš„ï¼Œæ‰€ä»¥ä¾ç„¶ä¼šå¡ä½ï¼‰
- å®Œå…¨ä¸æ˜¯ gradient descent

------

# ğŸŸ© **(2) Difference Map (DM) â€“ é«˜çº§æŠ•å½±æ³•**

ä»£è¡¨ï¼š

- Difference Map (Elser 2003)
- RAARã€HPRã€DRï¼ˆDouglasâ€“Rachfordï¼‰

æ ¸å¿ƒæ€æƒ³ï¼š

- ä½¿ç”¨ä¸¤ä¸ªæŠ•å½±ï¼š
   [
   \Pi_A, \Pi_B
   ]
- ç”¨åå°„è¿ç®— + å·®åˆ†æ„é€ è¿­ä»£ï¼š
   [
   x_{n+1}=x_n+\beta(\Pi_A(f_B(x_n)) - \Pi_B(f_A(x_n)))
   ]

å…¶ä¸­ (f_A, f_B) æ˜¯åå°„ç®—å­ã€‚

ç‰¹ç‚¹ï¼š

- **çº¯å‡ ä½•æŠ•å½±æ–¹æ³•ï¼Œä¸å±äºä¼˜åŒ–ç®—æ³•**
- ä¹Ÿæ˜¯**æ— å…¨å±€ loss function**
- å¾ˆç¨³å®šï¼Œå¾ˆæœ‰ç†è®ºèƒŒæ™¯ï¼ˆä¸å‡¸ä¼˜åŒ–ä¸­çš„ DR/ADMM æœ‰è”ç³»ï¼‰
- åœ¨ CDI ä¸­ä»ç„¶å¸¸ç”¨

------

# ğŸŸ© **(3) PIE familyï¼ˆPIE / ePIE / rPIE / AD-PIEï¼‰â€” åŠæŠ•å½±åŠæœ€å°äºŒä¹˜ï¼ˆpseudo-gradientï¼‰**

è¿™æ˜¯ä½ æœ€å…³å¿ƒçš„éƒ¨åˆ†ã€‚

PIE æœ¬è´¨å¹¶ä¸å±äº optimization familyï¼Œè€Œæ˜¯ï¼š

ğŸŒŸ **æŠ•å½± + least-squares å±€éƒ¨ä¿®æ­£**
 ğŸŒŸ **æ²¡æœ‰å…¨å±€è¯¯å·®å‡½æ•°ï¼ˆglobal lossï¼‰**
 ğŸŒŸ **åªå¯¹ diffraction constraint åšæŠ•å½±**
 ğŸŒŸ **overlap consistency æ˜¯ heuristic least-squares correction**

ç»“æ„ï¼š

1. å¯¹å•ä¸ª probe ä½ç½®çš„ exit wave åšæ¨¡æŠ•å½±
    [
    \Psi \to \Psi'=\sqrt{I}e^{i\angle\Psi}
    ]
2. ç”¨ overlap structure æŠŠè¯¯å·®åæ¨å› object/probeï¼š
    [
    O \leftarrow O + \alpha\frac{P^*}{|P|^2}(\Psi' - \Psi)
    ]

ç‰¹ç‚¹ï¼š

- **åªæœ‰ä¸€ä¸ªçº¦æŸç”¨äº†æŠ•å½±ï¼ˆæ¨¡çº¦æŸï¼‰**
- overlap consistency ä¸æ˜¯æŠ•å½±ï¼Œæ˜¯ heuristic correction
- æ›´æ–°æ–¹å‘ç­‰ä»·äºå±€éƒ¨è´Ÿæ¢¯åº¦ï¼ˆä½†ä¸æ˜¯ä» global loss å¾—æ¥ï¼‰
- **ä¸æ˜¯æœ€ä¼˜åŒ–æ–¹æ³•**
- å±äº **pseudo-gradient descent**

ğŸ‘‰ æ€»ç»“ï¼š
 **PIE æ˜¯ Projection ç±»æ–¹æ³•ï¼Œä¸æ˜¯ Optimization ç±»æ–¹æ³•ã€‚**
 åªæ˜¯å®ƒçš„å±€éƒ¨è¡Œä¸ºâ€œçœ‹èµ·æ¥åƒâ€ gradientã€‚

------

# ğŸŸ¥ **â…¡. ä¼˜åŒ–ç±»ï¼ˆOptimization-basedï¼‰ç®—æ³•**

> åŸºæœ¬æ€æƒ³ï¼š
>  å†™ä¸€ä¸ªçœŸæ­£çš„ **global loss function**ï¼ˆå…¨å±€ä»£ä»·å‡½æ•°ï¼‰ï¼Œ
>  ç”¨ gradient descent / Newton / ADMM å»æœ€å°åŒ–å®ƒã€‚

è¿™æ˜¯ 2010â€“2025 å¹´æ‰€æœ‰ç°ä»£é«˜è´¨é‡ ptychography çš„ä¸»çº¿æ€æƒ³ã€‚

------

# ğŸŸ¥ **(1) ML-based ptychographyï¼ˆMLPIE / Wirtinger gradientï¼‰**

ä»£è¡¨ï¼š

- Maximum Likelihood Ptychography (Thibault, Maiden, Rodenburg, Chen, Pennycook, etc.)
- Wirtinger Flow / gradient descent
- Poisson-likelihood ptychography
- Bayesian ptychography
- PtychoNN / PtychoCNNï¼ˆoptimization + learningï¼‰

æ ¸å¿ƒæ€æƒ³æ˜¯å†™ä¸€ä¸ª **å…¨å±€ loss**ï¼š

### Gaussian noise model:

[
 L = \sum_j \big( |\Psi_j| - \sqrt{I_j} \big)^2
 ]

### æˆ– Poisson noise model:

[
 L = \sum_j \left( |\Psi_j|^2 - I_j \log|\Psi_j|^2 \right)
 ]

ç„¶åå¯¹ Oã€P æ±‚æ¢¯åº¦ï¼š

[
 \nabla_O L = \sum_j (\text{Wirtinger derivative})
 ]

å¹¶ç”¨ï¼š

- gradient descent
- conjugate gradient
- ADAM
- stochastic gradient

æ¥æ›´æ–°ã€‚

ç‰¹ç‚¹ï¼š

- **æœ‰å…¨å±€ lossï¼ˆçœŸæ­£æ„ä¹‰ä¸Šçš„ä¼˜åŒ–é—®é¢˜ï¼‰**
- ç†è®ºä¸¥è°¨ï¼ˆML estimationã€ç»Ÿè®¡ä¼˜åŒ–ï¼‰
- æ¯” PIE æ›´ç¨³å®šï¼ˆå°¤å…¶æ··åˆæ€ã€å™ªå£°ã€åå·®æ¨¡å‹ï¼‰
- å¯ä»¥ç”¨ Hessian / Fisher ä¿¡æ¯
- é€‚åˆ electron ptychoï¼ˆ4D-STEM, mixed-stateï¼‰

ğŸ‘‰ æ€»ç»“ï¼š
 **MLPIEã€Wirtinger-based ptychography æ˜¯çœŸæ­£çš„ gradient å®¶æ—ã€‚
 ä¸æ˜¯æŠ•å½±æ³•ã€‚**

------

# ğŸŒ• **æœ€ç»ˆåˆ†ç±»æ€»è¡¨ï¼ˆä½ å¿…é¡»è®°ä½ï¼‰**

| ç±»åˆ«                          | ä¾‹å­                         | æ€æƒ³æ¥æº          | æ˜¯å¦æœ‰å…¨å±€ Loss? | æ˜¯å¦çœŸ Gradient?                | ç‰¹ç‚¹                     |
| ----------------------------- | ---------------------------- | ----------------- | ---------------- | ------------------------------- | ------------------------ |
| **æŠ•å½±ç±» POCS**               | ER, HIO, RAAR                | å‡ ä½•æŠ•å½±          | âŒ æ—              | âŒ éæ¢¯åº¦                        | æœ€å¤è€ï¼Œç¨³ä½†æ˜“åœæ»       |
| **Difference Map ç±»**         | DM, RAAR, HPR                | æŠ•å½± + åå°„       | âŒ æ—              | âŒ éæ¢¯åº¦                        | å¼ºå¤§çš„æŠ•å½±æ¡†æ¶           |
| **PIE family**                | PIE, ePIE, rPIE, AD-PIE      | æŠ•å½± + å±€éƒ¨LSä¿®æ­£ | âŒ æ— å…¨å±€loss     | âŒ églobal GDï¼ˆåªæ˜¯å±€éƒ¨è´Ÿæ¢¯åº¦ï¼‰ | æœ€å¸¸ç”¨ï¼Œå¿«ä½† heuristic   |
| **Optimization / MLï¼ˆç°ä»£ï¼‰** | MLPIE, Wirtinger, Poisson ML | ç»Ÿè®¡ä¼˜åŒ–          | âœ” æœ‰             | âœ” çœŸæ­£æ¢¯åº¦ä¸‹é™                  | ç†è®ºæœ€ä¸¥è°¨ã€é€‚åˆé«˜ç«¯å®éªŒ |

------

# ğŸŒˆ æœ€åä¸€å¥æœ€é‡è¦çš„æ€»ç»“

> **ER/DM/PIE = æŠ•å½±ç³»**
>  **MLPIE/Wirtinger = ä¼˜åŒ–ç³»**

PIE è™½ç„¶â€œæ–¹å‘åƒæ¢¯åº¦â€ï¼Œä½† **ä¸æ˜¯** optimization familyã€‚
 åªæœ‰ ML-based æ‰æ˜¯â€œçœŸæ­£æ„ä¹‰çš„ gradient descent familyâ€ã€‚

------

å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘è¿˜å¯ä»¥ä¸ºä½ ï¼š

ğŸ“Œ ç”»ä¸€å¼  **æ•´å¥—ç®—æ³•æ—è°±å›¾ï¼ˆFlowchart + å…³ç³»å›¾ï¼‰**
 ğŸ“Œ ç»™ä½ åšä¸€ä¸ªâ€œå¦‚ä½•ä» PIE è¿‡æ¸¡åˆ° MLPIE çš„ç†è§£æ¢¯åº¦â€
 ğŸ“Œ ç»™ä½ åšâ€œä¸ºä»€ä¹ˆ electron ptychography å¿…é¡»ç”¨ MLï¼ˆæ··åˆæ€éœ‡è¡ä¸¥é‡ï¼‰â€
 ğŸ“Œ ç»™ä½ åšâ€œä½ è‡ªå·±çš„ adaptive-z PIE å±äºå“ªä¸ª familyï¼Ÿâ€

ä½ æƒ³è¦å“ªä¸€ä¸ªï¼Ÿ



```
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚   Ptychography Algorithm Philosophy Tree     â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ptychography
â”‚
â”œâ”€â”€ â¶ Projection-based Methods (å‡ ä½•æŠ•å½±å®¶æ—)
â”‚       â”‚
â”‚       â”œâ”€â”€ (A) Classical POCS (Projection Onto Constraint Sets)
â”‚       â”‚       â”œâ”€â”€ ER  (Error Reduction)
â”‚       â”‚       â”œâ”€â”€ HIO (Hybrid Inputâ€“Output)
â”‚       â”‚       â”œâ”€â”€ RAAR / HPR / OSS
â”‚       â”‚       â””â”€â”€ (used mostly in CDI, not scan-based ptycho)
â”‚       â”‚
â”‚       â”œâ”€â”€ (B) Difference Map Family
â”‚       â”‚       â”œâ”€â”€ Difference Map (DM)
â”‚       â”‚       â”œâ”€â”€ RAAR (also fits here)
â”‚       â”‚       â”œâ”€â”€ HPR / Hybrid Projection-Reflection
â”‚       â”‚       â””â”€â”€ Douglasâ€“Rachford / ADMM-like projection operators
â”‚       â”‚
â”‚       â””â”€â”€ (C) PIE Family (Pseudoâ€“gradient projections)
â”‚               â”œâ”€â”€ PIE (2009)
â”‚               â”œâ”€â”€ ePIE
â”‚               â”œâ”€â”€ rPIE
â”‚               â”œâ”€â”€ AD-PIE (adaptive damping)
â”‚               â”œâ”€â”€ mPIE (momentum-like updates)
â”‚               â””â”€â”€ Mixed-state PIE (heuristic multi-mode)
â”‚
â”œâ”€â”€ â· Optimization-based Methods (æœ‰å…¨å±€ loss çš„çœŸæ­£ä¼˜åŒ–å®¶æ—)
â”‚       â”‚
â”‚       â”œâ”€â”€ (A) Maximum-Likelihood Ptychography
â”‚       â”‚       â”œâ”€â”€ Gaussian ML (amplitude error)
â”‚       â”‚       â”œâ”€â”€ Poisson ML (Thibault, Rodenburg, Maiden, etc.)
â”‚       â”‚       â”œâ”€â”€ Wirtinger gradient descent
â”‚       â”‚       â”œâ”€â”€ Conjugate-gradient ML
â”‚       â”‚       â”œâ”€â”€ Adam/SGD-based ML ptychography
â”‚       â”‚       â””â”€â”€ Mixed-state maximum-likelihood (Chen et al. Nat Comm 2020)
â”‚       â”‚
â”‚       â”œâ”€â”€ (B) Bayesian / Regularized Reconstruction
â”‚       â”‚       â”œâ”€â”€ MAP ptychography
â”‚       â”‚       â”œâ”€â”€ TV / sparsity / prior-based ptychography
â”‚       â”‚       â”œâ”€â”€ Variational inference ptychography
â”‚       â”‚       â””â”€â”€ EM-based mixed-state estimation
â”‚       â”‚
â”‚       â””â”€â”€ (C) Deep-learning-assisted Optimization
â”‚               â”œâ”€â”€ PtychoNN / PtychoML
â”‚               â”œâ”€â”€ Physics-informed neural ptychography
â”‚               â”œâ”€â”€ Ptychoformer (Transformer-based)
â”‚               â””â”€â”€ Learned priors + ML gradient (plug-and-play ADMM)
â”‚
â””â”€â”€ â¸ Hybrid Approaches (Projection Ã— Optimization æ··åˆï¼‰
        â”‚
        â”œâ”€â”€ ADMM-based Ptychography
        â”œâ”€â”€ HIO + ML hybrid methods
        â”œâ”€â”€ Projected-gradient ML (POCS + gradient)
        â””â”€â”€ Alternating minimization with projections

```

